{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Yang Hu\n",
    "1. This file gives a script to constrain cosmological parameters (H0, Omega0, Omegak, MB, etc.)\n",
    "using strong gravitational lenses (lenses) and type Ia supernovae (SNe) with non-parametric analysis.\n",
    "2. We use baryon accoustic oscillation data (BAO) to obtain an interpolated curve for Hubble parameter Hz via \n",
    "Gaussian Process (GP). GP is carried out using george package.\n",
    "3. For lenses, we use simulated LSST data. For SNe, we use simulated Roman data. For BAO, we use simulated DESI data.\n",
    "4. Constraints on parameters are obtained via Markov Chain Monte Carlo (MCMC) using emcee package.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "standard imports for data analysis; astropy.cosmology to compute astrophysical quantities with ease; george to run\n",
    "GP and emcee to run MCMC\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.integrate import simps\n",
    "from scipy.optimize import minimize\n",
    "import corner\n",
    "\n",
    "import george\n",
    "from george import kernels\n",
    "import emcee \n",
    "\n",
    "from astropy.cosmology import FlatLambdaCDM, FlatwCDM, LambdaCDM, wCDM, w0waCDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load original data. Please check that the relevant data files are named and stored in the way the code below \n",
    "specified.\n",
    "\"\"\"\n",
    "\n",
    "#load&rename lenses data\n",
    "data1 = pd.read_csv(\"Data/zlens_zsource_LSSTLike.txt\", delimiter=' ', header=None)\n",
    "zlens = data1[0]\n",
    "zsource = data1[1]\n",
    "ddt_err = data1[2]\n",
    "\n",
    "#load&rename SNe data\n",
    "data2 = pd.read_csv(\"Data/lcparam_WFIRST_G10.txt\", delimiter=' ', skiprows=1, header=None)\n",
    "data3 = pd.read_csv(\"Data/syscov_WFIRST.txt\", delimiter=' ', skiprows=0, header=None)\n",
    "zcmb = data2[1]\n",
    "mb_err = data2[5]\n",
    "sys_err = np.array(data3)\n",
    "D_stat = np.diag(mb_err**2)\n",
    "C_sys = sys_err.reshape((len(data2), len(data2)))\n",
    "C = D_stat + C_sys\n",
    "C_inv = np.linalg.inv(C)\n",
    "\n",
    "#load&rename BAO data\n",
    "data4 = pd.read_csv(\"Data/DESI_HZ_error.txt\", delimiter=' ', skiprows=1, header=None) \n",
    "zBAO = data4[0] #x\n",
    "sigHz = data4[1] #yerr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sometimes we want more simulated data for lenses. Here we provide a way to genearate more LSST-like data.\n",
    "The number of lensing events in the original LSST-like data is 310, an estimate of expected observed number\n",
    "of events in LSST's 10-year survey baseline.\n",
    "\"\"\"\n",
    "\n",
    "#decide whether to generate data and uncertainty for lenses and whether to use them\n",
    "generate_data = False\n",
    "generate_uncertainty = False\n",
    "save_data = False\n",
    "#IMPORTANT!!!\n",
    "#if use_data switch is on, and number of lenses is other than 310, then we must use alterative options in name, \n",
    "#ddt_err below and looping in log likelihood function!!!\n",
    "use_data = False\n",
    "\n",
    "\n",
    "#first entry for number is the total number of data points we want, second entry is the number of original data\n",
    "#IMPORTANT!!!\n",
    "#if number of lenses is other than 310, then we must use alterative options in name, ddt_err below and looping \n",
    "#in log likelihood function!!!\n",
    "number = 3000-310\n",
    "#choose a python random seed for random processes involved\n",
    "seed_no = 22\n",
    "\n",
    "if generate_uncertainty:\n",
    "    np.random.seed(seed_no)\n",
    "    pu = np.random.uniform(0.06, 0.1, size=len(data1[0]))\n",
    "\n",
    "if generate_data:\n",
    "    np.random.seed(seed_no)\n",
    "    rand_number = np.random.randint(0, 309, size=number)\n",
    "\n",
    "if save_data:\n",
    "    data_temp = np.array(data1)\n",
    "    data_temp[:, 2]=pu\n",
    "    for i in rand_number:\n",
    "        data_temp = np.append(data_temp, [data_temp[i]], axis=0)\n",
    "    df = pd.DataFrame(np.concatenate(([data_temp[:, 0]], [data_temp[:, 1]], [data_temp[:, 2]])).T)\n",
    "    df.to_csv(r'Data/zlens_zsource_%sLSSTLike_%s.csv' % ((number+310), seed_no), index=False)\n",
    "\n",
    "if use_data:\n",
    "    data_new = pd.read_csv(\"Data/zlens_zsource_%sLSSTLike_%s.csv\" % ((number+310), seed_no), skiprows=1, header=None)\n",
    "    zlens = data_new[0]\n",
    "    zsource = data_new[1]\n",
    "    #alternative\n",
    "    #ddt_err = data_new[2]/np.sqrt(10)\n",
    "    ddt_err = data_new[2]\n",
    "    #alternative\n",
    "    #name = number+310\n",
    "    name = ''    \n",
    "else:\n",
    "    zlens = data1[0]\n",
    "    zsource = data1[1]\n",
    "    ddt_err = data1[2]\n",
    "    name = ''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Since in the original DESI data file we do not have mock Hz values, we still have to generate Hz values by\n",
    "assuming a mock universe model. If the simulated BAO data file contains measured Hz values, we do not need to do \n",
    "so.\n",
    "The same situation applies to lenses and SNe.\n",
    "\"\"\"\n",
    "\n",
    "#compute mock Hz from BAO data\n",
    "H0_mock, Om0_mock, Ok0_mock, w_mock, MB_mock = 72, 0.3, 0.00, -1, -19.2\n",
    "cosmo_mock = wCDM(H0=H0_mock, Om0=Om0_mock, Ode0=1.-Om0_mock-Ok0_mock, w0=w_mock)\n",
    "Hz_mock = cosmo_mock.H(z=zBAO).value #y for GP\n",
    "\n",
    "#compute mock time-delay distance for lenses\n",
    "dd_mock = cosmo_mock.angular_diameter_distance(z=zlens).value\n",
    "ds_mock = cosmo_mock.angular_diameter_distance(z=zsource).value\n",
    "dds_mock = cosmo_mock.angular_diameter_distance_z1z2(z1=zlens, z2=zsource).value\n",
    "ddt_mock = (1. + zlens) * dd_mock * ds_mock / dds_mock\n",
    "\n",
    "#compute mock luminosity distance for SNe\n",
    "dl_mock = cosmo_mock.luminosity_distance(z=zcmb).value\n",
    "mb_mock = 5*np.log10(dl_mock)+25+MB_mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GP interpolate Hz from BAO data using george package\n",
    "\"\"\"\n",
    "\n",
    "#function to get GP interpolation\n",
    "def get_gp_interp(zBAO, Hz_mock, sigHz):\n",
    "    \"\"\"\n",
    "    Compute the GP interpolation with the error covariance matrix\n",
    "    TO DO: Marginalise over kernel hyperparameters\n",
    "    zBAO: array of z from BAO\n",
    "    Hz_mock: array of Hz from BAO\n",
    "    sigHz: array of uncertainty of Hz\n",
    "    \"\"\"\n",
    "    #the 2 optical numerical values of hyperparameters are obtained via MCMC in a seperate script \"HzGP_constraint.ipynb\"\n",
    "    gp = george.GP(10.186*np.var(Hz_mock)*kernels.ExpSquaredKernel(1.939),\n",
    "               mean=np.mean(Hz_mock), white_noise=None)\n",
    "    Hz = Hz_mock\n",
    "    Hz_err = sigHz\n",
    "    #define the redshift region for predicting the GP fit\n",
    "    z_step = np.linspace(0., max(zBAO), 4000)\n",
    "    gp.compute(zBAO, Hz_err)\n",
    "    Hz_gp = gp.predict(Hz, z_step, return_cov=False)\n",
    "    return z_step, Hz_gp\n",
    "\n",
    "z_step, Hz_gp = get_gp_interp(zBAO, Hz_mock, sigHz)\n",
    "#normalise the E(z)\n",
    "Ez_gp = Hz_gp/Hz_gp[0]\n",
    "#this is the normalised Hz interpolation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We want to get cosmological distances in likelihood function in a non-parametric way.  \n",
    "\"\"\"\n",
    "\n",
    "c = 299792.458 #in km/s\n",
    "\n",
    "#function to get time-delay distance in a non-parametric way\n",
    "def get_model_indep_dist_ddt(zl, zs, z_step, Ez_gp, h0, ok):\n",
    "    \"\"\"\n",
    "    Doesn't account for errors in the GP fit as of now\n",
    "    zl: float of z at lenses\n",
    "    zs: float of z at source to integrate till\n",
    "    z_step: float, the baseline step for integrating\n",
    "    Ez_gp: array of normalised Hubble parameter from GP interpolation\n",
    "    h0: float, a value of Hubble's constant\n",
    "    ok: float, a value of curvature density parameter\n",
    "    \"\"\"\n",
    "    cond_l = z_step <= zl\n",
    "    cond_s = z_step <= zs\n",
    "    Ez_gp_l = Ez_gp[cond_l]\n",
    "    Ez_gp_s = Ez_gp[cond_s]\n",
    "    dh = c/h0\n",
    "    if ok < 0.:\n",
    "        int_comov_l = simps(1/Ez_gp_l, z_step[cond_l])\n",
    "        dm_l = dh/np.sqrt(abs(ok))*np.sin(np.sqrt(abs(ok))*int_comov_l)\n",
    "        da_l = dm_l/(1+zl)\n",
    "        int_comov_s = simps(1/Ez_gp_s, z_step[cond_s])\n",
    "        dm_s = dh/np.sqrt(abs(ok))*np.sin(np.sqrt(abs(ok))*int_comov_s)\n",
    "        da_s = dm_s/(1+zs)\n",
    "        da_ls = (1/(1+zs)*\n",
    "        (dm_s*np.sqrt(1+ok*(dm_l/dh)**2)-dm_l*np.sqrt(1+ok*(dm_s/dh)**2))\n",
    "                )\n",
    "    elif ok == 0.:\n",
    "        int_comov_l = simps(1/Ez_gp_l, z_step[cond_l])\n",
    "        dm_l = dh*int_comov_l\n",
    "        da_l = dm_l/(1+zl)\n",
    "        int_comov_s = simps(1/Ez_gp_s, z_step[cond_s])\n",
    "        dm_s = dh*int_comov_s\n",
    "        da_s = dm_s/(1+zs)\n",
    "        da_ls = 1/(1+zs)*(dm_s-dm_l)\n",
    "    elif ok > 0.:\n",
    "        int_comov_l = simps(1/Ez_gp_l, z_step[cond_l])\n",
    "        dm_l = dh/np.sqrt(abs(ok))*np.sinh(np.sqrt(abs(ok))*int_comov_l)\n",
    "        da_l = dm_l/(1+zl)\n",
    "        int_comov_s = simps(1/Ez_gp_s, z_step[cond_s])\n",
    "        dm_s = dh/np.sqrt(abs(ok))*np.sinh(np.sqrt(abs(ok))*int_comov_s)\n",
    "        da_s = dm_s/(1+zs)\n",
    "        da_ls = (1/(1+zs)*\n",
    "        (dm_s*np.sqrt(1+ok*(dm_l/dh)**2)-dm_l*np.sqrt(1+ok*(dm_s/dh)**2))\n",
    "                )\n",
    "    ddt = (1.+zl) * da_l * da_s / da_ls\n",
    "    return ddt\n",
    "\n",
    "#function to get luminosity distance in a non-parametric way\n",
    "def get_model_indep_dist_dl(z, z_step, Ez_gp, h0, ok):\n",
    "    \"\"\"\n",
    "    Doesn't account for errors in the GP fit as of now\n",
    "    z: float of z from SNe \n",
    "    z_step: float, the baseline step for integrating\n",
    "    Ez_gp: array of normalised Hubble parameter from GP interpolation\n",
    "    h0: float, a value of Hubble's constant\n",
    "    ok: float, a value of curvature density parameter\n",
    "    \"\"\"\n",
    "    cond = z_step <= z\n",
    "    Ez_gp = Ez_gp[cond]\n",
    "    if ok < 0.:\n",
    "        int_comov = simps(1/Ez_gp, z_step[cond])\n",
    "        comov_curv = np.sqrt(abs(ok)) * int_comov\n",
    "        dl_int = c*np.sin(comov_curv) * (1+z) / (np.sqrt(abs(ok)))      \n",
    "    elif ok > 0.:\n",
    "        int_comov = simps(1/Ez_gp, z_step[cond])\n",
    "        comov_curv = np.sqrt(abs(ok)) * int_comov      \n",
    "        dl_int = c*np.sinh(comov_curv) * (1+z) / (np.sqrt(abs(ok)))  \n",
    "    elif ok == 0.:\n",
    "        int_comov = simps(1/Ez_gp, z_step[cond])\n",
    "        comov_curv = int_comov\n",
    "        dl_int = c*comov_curv * (1+z)\n",
    "    dl_int /= h0\n",
    "    return dl_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Constraints are obtained by MCMC and here we define the relevant prior and likelihood functions \n",
    "\"\"\"\n",
    "\n",
    "#decide whether to run a maximum likelihood testing before running the full lengthy MCMC analysis\n",
    "ml_test = True\n",
    "\n",
    "#use uniform priors for all parameters\n",
    "def log_prior(theta):\n",
    "    \"\"\"\n",
    "    theta: list of floats, folded cosmological parameters.\n",
    "    \"\"\"\n",
    "    h0, ok, Mb = theta\n",
    "    if 0. <= h0 <= 150. and -0.5 <= ok <= 0.5 and -38.4 <= Mb <= 0.:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return -np.inf\n",
    "\n",
    "#use a chi-square likelihood function\n",
    "def log_likelihood(theta, zlens, zsource, ddt_err, zcmb, C_inv, z_step, Ez_gp): \n",
    "    \"\"\"\n",
    "    theta: list of floats, folded cosmological parameters.\n",
    "    zlens: array of z at lens\n",
    "    zsource: array of z at source\n",
    "    ddt_err: array of uncertainty of time-delay distance\n",
    "    zcmb: array of z of cmb, obtained for SNe data\n",
    "    C_inv: covariance matrix indicating uncertainty of SNe data\n",
    "    z_step: float, the baseline step for integrating\n",
    "    Ez_gp: array of normalised Hubble parameter from GP interpolation\n",
    "    \"\"\"    \n",
    "    h0, ok, Mb = theta\n",
    "    zl_list = list(zlens)\n",
    "    zs_list = list(zsource)\n",
    "    ddt_list = [0]*len(zlens)\n",
    "    for n in range(len(zlens)):\n",
    "        zl = zl_list[n]\n",
    "        zs = zs_list[n]\n",
    "        #alternative looping (slow!!!)\n",
    "        #zs = zsource[z_list_ddt.index(zl)]\n",
    "        #ddt[z_list_ddt.index(zl)] = get_model_indep_dist(zl, zs, z_step, Ez_gp, h0, ok)\n",
    "        ddt_list[n] = get_model_indep_dist_ddt(zl, zs, z_step, Ez_gp, h0, ok)\n",
    "    ddt = np.array(ddt_list)\n",
    "    z_list_dl = list(zcmb)\n",
    "    dl = np.zeros(len(zcmb))\n",
    "    for z in zcmb:        \n",
    "        dl[z_list_dl.index(z)] = get_model_indep_dist_dl(z, z_step, Ez_gp, h0, ok)\n",
    "    #check parameters are physical\n",
    "    if np.all(ddt >= 0):\n",
    "        if np.all(dl >=0):\n",
    "            mb = 5*np.log10(dl)+25+Mb\n",
    "            del_m = mb_mock - mb\n",
    "            chi_sq_SNe = np.dot(del_m.T, np.dot(C_inv, del_m))\n",
    "            chi_sq_lenses = np.sum((ddt-ddt_mock)**2./(ddt*ddt_err)**2.)\n",
    "            return -0.5*(chi_sq_SNe+chi_sq_lenses)\n",
    "        else:\n",
    "            return -np.inf\n",
    "    else:\n",
    "        return -np.inf\n",
    "\n",
    "def log_probability(theta, zlens, zsource, ddt_err, zcmb, C_inv, z_step, Ez_gp):\n",
    "    \"\"\"\n",
    "    theta: list of floats, folded cosmological parameters.\n",
    "    zlens: array of z at lens\n",
    "    zsource: array of z at source\n",
    "    ddt_err: array of uncertainty of time-delay distance\n",
    "    zcmb: array of z of cmb, obtained for SNe data\n",
    "    C_inv: covariance matrix indicating uncertainty of SNe data\n",
    "    z_step: float, the baseline step for integrating\n",
    "    Ez_gp: array of normalised Hubble parameter from GP interpolation\n",
    "    \"\"\"    \n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, zlens, zsource, ddt_err, zcmb, C_inv, z_step, Ez_gp)\n",
    "\n",
    "#run a maximum likelihood test\n",
    "if ml_test:\n",
    "    nll = lambda *args: -log_likelihood(*args)\n",
    "    initial = np.array([75., 0.02, -19.])\n",
    "    soln = minimize(nll, initial, args=(zlens, zsource, ddt_err, zcmb, C_inv, z_step, Ez_gp))\n",
    "    H0_ml, Ok_ml, Mb_ml = soln.x\n",
    "    print(\"Maximum likelihood estimates:\")\n",
    "    print(\"H0_ml = {0:.3f}\".format(H0_ml))\n",
    "    print(\"Ok_ml = {0:.3f}\".format(Ok_ml))\n",
    "    print(\"MB_ml = {0:.3f}\".format(Mb_ml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "presettings for MCMC\n",
    "\"\"\"\n",
    "\n",
    "nwalkers = 16\n",
    "nsamples = 3000\n",
    "\n",
    "startpos = [70., 0.02, -19.]  # H0, Ok\n",
    "labels = [\"H0\", \"Ok\", \"MB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "run MCMC using emcee package with the presetting above\n",
    "\"\"\"\n",
    "\n",
    "#decide whether to run MCMC and save the obtained samples, and whether to load a certain sample file\n",
    "run_mcmc = True\n",
    "save_mcmc = True\n",
    "load_mcmc = True\n",
    "\n",
    "if run_mcmc:\n",
    "    pos = startpos + 1e-4 * np.random.randn(nwalkers, len(startpos))\n",
    "    nwalkers, ndim = pos.shape\n",
    "    display(\"Sampling cosmological parameters...\")\n",
    "    sampler = emcee.EnsembleSampler(\n",
    "        nwalkers, ndim, log_probability, args=[zlens, zsource, ddt_err, zcmb, C_inv, z_step, Ez_gp]\n",
    "    )\n",
    "    sampler.run_mcmc(pos, nsamples, progress=True);\n",
    "\n",
    "if save_mcmc:\n",
    "    samples = sampler.get_chain()\n",
    "    flat_samples = sampler.get_chain(discard=500, thin=4, flat=True)\n",
    "    sample_data = pd.DataFrame(flat_samples)\n",
    "    \n",
    "    sample_data.to_csv(\n",
    "        \"GP_samples/%sLSST+Roman_HzGP_%ix%i.csv\" \n",
    "        % (name, nwalkers, nsamples), \n",
    "        index=False, header=labels\n",
    "    )\n",
    "\n",
    "if load_mcmc:\n",
    "    flat_samples = pd.read_csv(\"GP_samples/%sLSST+Roman_HzGP_%ix%i.csv\" \n",
    "                               % (name, nwalkers, nsamples), skiprows=1, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After loading a sample file, we can plot a corner plot showing how well constraints are placed on each interested\n",
    "cosmological paramters by the data.\n",
    "\"\"\"\n",
    "\n",
    "#corner plot\n",
    "fig = corner.corner(\n",
    "    flat_samples, labels=labels, quantiles=(0.16, 0.84), show_titles=True, \n",
    "    title_fmt='.3f', use_math_text=True, truths=[H0_mock, Ok0_mock, MB_mock]\n",
    ")\n",
    "fig.suptitle('Corner plot, %sLSST + Roman, Hz GP fit, %ix%i' % (name, nwalkers, nsamples),\n",
    "             y=1.02)\n",
    "fig.savefig(\"GP_plots/%sLSST+Roman_HzGP_%ix%i_corner.png\" \n",
    "            % (name, nwalkers, nsamples)\n",
    "            , bbox_inches='tight'\n",
    "           )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
